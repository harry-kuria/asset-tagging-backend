name: üöÄ Deploy Asset Tagging Backend

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      deploy_only:
        description: 'Skip Terraform and deploy to existing EC2 only'
        required: false
        type: boolean
        default: false
      ec2_public_ip:
        description: 'EC2 public IP for deploy-only runs (overrides secret if provided)'
        required: false
        type: string
        default: ''
      rds_endpoint:
        description: 'RDS endpoint for migration/health (overrides secret if provided)'
        required: false
        type: string
        default: ''
      load_balancer_dns:
        description: 'ALB DNS for health checks (overrides secret if provided)'
        required: false
        type: string
        default: ''
      ssh_user:
        description: 'SSH username for EC2 (e.g., ubuntu, ec2-user)'
        required: false
        type: string
        default: 'ubuntu'

env:
  AWS_REGION: us-east-1
  ECR_REPOSITORY: asset-tagging-backend
  INFRASTRUCTURE_REPO: harry-kuria/moowi-IAC

jobs:
  # Test the backend application
  test:
    name: üß™ Test Backend
    runs-on: ubuntu-latest
    
    steps:
    - name: üì• Checkout code
      uses: actions/checkout@v4
      
    - name: üêπ Setup Go
      uses: actions/setup-go@v4
      with:
        go-version: '1.22.x'
        cache: true
        
    - name: üì¶ Install dependencies
      run: go mod download
      
    - name: üß™ Run tests
      run: go test -v ./...
      
    - name: üîç Run linting
      run: |
        go install golang.org/x/lint/golint@latest
        golint -set_exit_status ./...
        
    - name: üîí Run security scan
      run: |
        go env -w GOFLAGS='-buildvcs=false'
        go install github.com/securego/gosec/v2/cmd/gosec@v2.20.0
        echo "$HOME/go/bin" >> $GITHUB_PATH
        gosec ./...

  # Build and push Docker image
  build:
    name: üê≥ Build & Push Docker Image
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop' || github.event_name == 'workflow_dispatch'
    
    steps:
    - name: üì• Checkout code
      uses: actions/checkout@v4
      
    - name: üîê Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: üèóÔ∏è Ensure ECR repository exists and is mutable
      run: |
        aws ecr describe-repositories --repository-names $ECR_REPOSITORY >/dev/null 2>&1 || \
        aws ecr create-repository \
          --repository-name $ECR_REPOSITORY \
          --image-tag-mutability MUTABLE \
          --image-scanning-configuration scanOnPush=true
        # Force mutability in case repo already existed and was immutable
        aws ecr put-image-tag-mutability \
          --repository-name $ECR_REPOSITORY \
          --image-tag-mutability MUTABLE || true
      
    - name: üîë Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2
      
    - name: üèóÔ∏è Build, tag, and push image to Amazon ECR
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        IMAGE_TAG: ${{ github.sha }}
      run: |
        docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG .
        docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:latest .
        docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
        docker push $ECR_REGISTRY/$ECR_REPOSITORY:latest
        echo "image=$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG" >> $GITHUB_OUTPUT

  # Deploy to production
  deploy:
    name: üö´ Provision Infrastructure (disabled)
    needs: build
    runs-on: ubuntu-latest
    if: false
    
    steps:
    - name: üì• Checkout infrastructure repo
      uses: actions/checkout@v4
      with:
        repository: ${{ env.INFRASTRUCTURE_REPO }}
        token: ${{ secrets.GH_PAT || github.token }}
        path: infrastructure
        ref: main
        
    - name: üîê Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
        
    - name: üßπ Pre-clean conflicting named resources (safe, idempotent)
      run: |
        set -e
        echo "Checking for pre-existing resources that may conflict with names..."
        ALB_ARN=$(aws elbv2 describe-load-balancers --region $AWS_REGION --names asset-tagging-alb --query 'LoadBalancers[0].LoadBalancerArn' --output text 2>/dev/null || echo "None")
        TG_ARN=$(aws elbv2 describe-target-groups --region $AWS_REGION --names asset-tagging-tg --query 'TargetGroups[0].TargetGroupArn' --output text 2>/dev/null || echo "None")
        # Delete listeners if ALB exists
        if [ "$ALB_ARN" != "None" ] && [ -n "$ALB_ARN" ]; then
          echo "Found existing ALB: $ALB_ARN. Deleting listeners..."
          LISTENERS=$(aws elbv2 describe-listeners --region $AWS_REGION --load-balancer-arn "$ALB_ARN" --query 'Listeners[].ListenerArn' --output text 2>/dev/null || true)
          for L in $LISTENERS; do
            aws elbv2 delete-listener --region $AWS_REGION --listener-arn "$L" || true
          done
          echo "Deleting ALB..."
          aws elbv2 delete-load-balancer --region $AWS_REGION --load-balancer-arn "$ALB_ARN" || true
          aws elbv2 wait load-balancer-deleted --region $AWS_REGION --load-balancer-arns "$ALB_ARN" || true
        else
          echo "No pre-existing ALB found."
        fi
        # Delete target group
        if [ "$TG_ARN" != "None" ] && [ -n "$TG_ARN" ]; then
          echo "Found existing TG: $TG_ARN. Deregistering targets and deleting TG..."
          aws elbv2 modify-target-group-attributes --region $AWS_REGION --target-group-arn "$TG_ARN" --attributes Key=deregistration_delay.timeout_seconds,Value=30 || true
          TARGET_IDS=$(aws elbv2 describe-target-health --region $AWS_REGION --target-group-arn "$TG_ARN" --query 'TargetHealthDescriptions[].Target.Id' --output text 2>/dev/null || true)
          for id in $TARGET_IDS; do
            aws elbv2 deregister-targets --region $AWS_REGION --target-group-arn "$TG_ARN" --targets Id=$id || true
          done
          sleep 10
          aws elbv2 delete-target-group --region $AWS_REGION --target-group-arn "$TG_ARN" || true
        else
          echo "No pre-existing TG found."
        fi
        # Delete DB subnet group
        echo "Attempting to delete pre-existing DB subnet group (if any)..."
        aws rds delete-db-subnet-group --region $AWS_REGION --db-subnet-group-name asset-tagging-db-subnet-group || true
        echo "Pre-clean complete."
        
    - name: üì¶ Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: "1.5.0"
        
    - name: üîç Terraform Format Check
      working-directory: ./infrastructure/terraform
      run: |
        terraform fmt -recursive
        terraform fmt -recursive -check -diff
      
    - name: ‚úÖ Terraform Init
      working-directory: ./infrastructure/terraform
      env:
        TF_IN_AUTOMATION: 'true'
        TF_VAR_db_password: ${{ secrets.DB_PASSWORD }}
        TF_VAR_ssh_key_name: ${{ secrets.SSH_KEY_NAME }}
        TF_VAR_instance_type: 't2.micro'
        TF_VAR_manage_db: 'false'
      run: terraform init
      
    - name: üîé Discover existing DB instance (if any)
      run: |
        set -e
        echo "Checking for pre-existing RDS instance with identifier asset-tagging-db..."
        DB_EXISTS=$(aws rds describe-db-instances --region $AWS_REGION --db-instance-identifier asset-tagging-db >/dev/null 2>&1 && echo yes || echo no)
        echo "DB_EXISTS=${DB_EXISTS}" >> $GITHUB_ENV
      
    - name: üß≠ Import existing DB instance (one-time)
      if: env.DB_EXISTS == 'yes'
      working-directory: ./infrastructure/terraform
      env:
        TF_IN_AUTOMATION: 'true'
        TF_INPUT: 'false'
        TF_VAR_db_password: ${{ secrets.DB_PASSWORD }}
        TF_VAR_ssh_key_name: ${{ secrets.SSH_KEY_NAME }}
      run: |
        terraform import aws_db_instance.main asset-tagging-db || true
        terraform init -reconfigure || true
      
    - name: üìã Terraform Plan
      working-directory: ./infrastructure/terraform
      env:
        TF_IN_AUTOMATION: 'true'
        TF_VAR_db_password: ${{ secrets.DB_PASSWORD }}
        TF_VAR_ssh_key_name: ${{ secrets.SSH_KEY_NAME }}
        TF_VAR_instance_type: 't2.micro'
        TF_VAR_manage_db: 'false'
      run: terraform plan -input=false -out=tfplan
      
    - name: üöÄ Terraform Apply
      working-directory: ./infrastructure/terraform
      env:
        TF_IN_AUTOMATION: 'true'
        TF_VAR_db_password: ${{ secrets.DB_PASSWORD }}
        TF_VAR_ssh_key_name: ${{ secrets.SSH_KEY_NAME }}
        TF_VAR_instance_type: 't2.micro'
        TF_VAR_manage_db: 'false'
      run: terraform apply -input=false -auto-approve tfplan
      
    # Terraform outputs step removed because provisioning is disabled

  # Deploy application to EC2
  deploy-application:
    name: üöÄ Deploy Application
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch'
    env:
      EC2_PUBLIC_IP: ${{ inputs.ec2_public_ip != '' && inputs.ec2_public_ip || secrets.EC2_PUBLIC_IP }}
      SSH_USER: ${{ inputs.ssh_user != '' && inputs.ssh_user || secrets.EC2_SSH_USER }}
    
    steps:
    - name: üì• Checkout infrastructure repo
      uses: actions/checkout@v4
      with:
        repository: ${{ env.INFRASTRUCTURE_REPO }}
        token: ${{ secrets.GH_PAT || github.token }}
        path: infrastructure
        ref: main
        
    - name: üîê Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
        
    - name: üîë Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2
      
    - name: üîë Setup SSH
      run: |
        set -e
        mkdir -p ~/.ssh
        if [ -z "${{ secrets.EC2_SSH_PRIVATE_KEY }}" ]; then echo "EC2_SSH_PRIVATE_KEY secret is required and not set."; exit 1; fi
        # Write key, handling both literal newlines and \n-escaped forms and CRLF
        rm -f ~/.ssh/id_rsa
        printf "%s" "${{ secrets.EC2_SSH_PRIVATE_KEY }}" | perl -pe 's/\r//g; s/\\n/\n/g' > ~/.ssh/id_rsa || true
        # If it still doesn't look like a key, try base64 decoding
        if ! grep -q "BEGIN .*PRIVATE KEY" ~/.ssh/id_rsa; then
          echo "Key does not appear to be raw PEM/OpenSSH. Trying base64 decode..."
          printf "%s" "${{ secrets.EC2_SSH_PRIVATE_KEY }}" | base64 -d > ~/.ssh/id_rsa 2>/dev/null || true
        fi
        chmod 600 ~/.ssh/id_rsa
        # Validate that the key can be loaded and is not passphrase-protected
        if ! ssh-keygen -y -f ~/.ssh/id_rsa > /dev/null 2>&1; then
          echo "Invalid SSH private key in secrets (format or passphrase). Ensure it is an unencrypted PEM/OpenSSH key with correct newlines.";
          exit 1
        fi
        : "${SSH_USER:=ubuntu}"
        if [ -z "${{ env.EC2_PUBLIC_IP }}" ]; then echo "EC2_PUBLIC_IP is not set. Configure the EC2_PUBLIC_IP secret."; exit 1; fi
        ssh-keyscan -H "${{ env.EC2_PUBLIC_IP }}" >> ~/.ssh/known_hosts
        
    - name: üìã Create deployment script
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
      run: |
        cat > deploy-to-server.sh << 'EOF'
        #!/bin/bash
        set -e
        
        echo "üöÄ Starting deployment..."
        
        # Update infrastructure repository (skip if git auth fails)
        cd ~/moowi-IAC
        if git remote -v | grep -q "https://github.com"; then
          echo "‚ö†Ô∏è  Infrastructure repo uses HTTPS, skipping git pull (auth required)"
        else
          git pull origin main || echo "‚ö†Ô∏è  Git pull failed, continuing with deployment"
        fi
        
        # Stop current services
        cd /opt/asset-tagging
        sudo docker-compose down || true

        echo "üßπ Pruning unused Docker images and containers to free space..."
        sudo docker system df || true
        sudo docker container prune -f || true
        sudo docker image prune -af || true
        sudo docker system df || true
        
        # Install AWS CLI if not present
        if ! command -v aws &> /dev/null; then
          echo "üì¶ Installing AWS CLI..."
          cd /tmp
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          unzip -q awscliv2.zip
          sudo ./aws/install --update
          rm -rf aws awscliv2.zip
          cd -
        fi
        
        # Login to ECR
        echo "üîê Logging into ECR..."
        aws ecr get-login-password --region us-east-1 | sudo docker login --username AWS --password-stdin ${ECR_REGISTRY}
        
        # Login to Docker Hub to avoid rate limits (if credentials available)
        if [ -n "${DOCKERHUB_USERNAME:-}" ] && [ -n "${DOCKERHUB_PASSWORD:-}" ]; then
          echo "üîê Logging into Docker Hub..."
          echo "${DOCKERHUB_PASSWORD}" | sudo docker login --username "${DOCKERHUB_USERNAME}" --password-stdin || true
        else
          echo "‚ö†Ô∏è  No Docker Hub credentials provided, may hit rate limits"
        fi
        
        # Pull latest image with proper ECR registry
        ECR_REGISTRY="${ECR_REGISTRY}"
        ECR_REPOSITORY="asset-tagging-backend"
        FULL_IMAGE="${ECR_REGISTRY}/${ECR_REPOSITORY}:latest"
        
        echo "üì¶ Pulling image: ${FULL_IMAGE}"
        sudo docker pull "${FULL_IMAGE}"
        
        # Ensure Docker Compose is available (prefer v2 plugin)
        if docker compose version >/dev/null 2>&1; then
          DC="docker compose"
        elif command -v docker-compose >/dev/null 2>&1; then
          DC="docker-compose"
        else
          echo "üì¶ Installing Docker Compose v2..."
          sudo mkdir -p /usr/local/lib/docker/cli-plugins
          ARCH=$(uname -m)
          COMPOSE_URL="https://github.com/docker/compose/releases/download/v2.27.0/docker-compose-linux-x86_64"
          if [ "$ARCH" = "aarch64" ] || [ "$ARCH" = "arm64" ]; then
            COMPOSE_URL="https://github.com/docker/compose/releases/download/v2.27.0/docker-compose-linux-aarch64"
          fi
          sudo curl -sSL "$COMPOSE_URL" -o /usr/local/lib/docker/cli-plugins/docker-compose
          sudo chmod +x /usr/local/lib/docker/cli-plugins/docker-compose
          DC="docker compose"
        fi
        
        # Resolve Grafana image with fallback (quay -> docker hub)
        GRAFANA_IMAGE="quay.io/grafana/grafana:latest"
        echo "üì¶ Pre-pulling Grafana from ${GRAFANA_IMAGE} (will fallback to Docker Hub if unauthorized)..."
        if ! sudo docker pull "${GRAFANA_IMAGE}" >/dev/null 2>&1; then
          echo "‚ö†Ô∏è  Unable to pull ${GRAFANA_IMAGE}, falling back to Docker Hub grafana/grafana:latest"
          GRAFANA_IMAGE="docker.io/grafana/grafana:latest"
          # Attempt Docker Hub login only if creds are present (already done earlier)
          sudo docker pull "${GRAFANA_IMAGE}" || true
        fi
        
        # Pre-pull public images to avoid rate limits
        sudo docker pull quay.io/prometheus/prometheus:latest || true
        sudo docker pull public.ecr.aws/docker/library/nginx:alpine || true
        
        # Create override atomically with sudo, expanding variables safely
        sudo bash -c "printf '%s\n' \
          'services:' \
          '  app:' \
          '    image: ${FULL_IMAGE}' \
          '  grafana:' \
          '    image: ${GRAFANA_IMAGE}' \
          '  prometheus:' \
          '    image: quay.io/prometheus/prometheus:latest' \
          '  nginx:' \
          '    image: public.ecr.aws/docker/library/nginx:alpine' \
          > docker-compose.override.yml"
        
        # Start services (this will pull other images as needed)
        echo "üöÄ Starting services..."
        sudo ${DC} -f docker-compose.yml -f docker-compose.override.yml up -d
        
        echo "‚è≥ Waiting for services to be ready..."
        for i in {1..10}; do
          if curl -fsS http://localhost:5000/health > /dev/null; then
            echo "‚úÖ Application deployed successfully!"
            exit 0
          fi
          echo "Attempt $i/10: service not ready yet, waiting 10s..."
          sleep 10
        done
        echo "‚ùå Application health check failed after retries!"
        exit 1
        EOF
        
    - name: üöÄ Deploy to EC2
      run: |
        chmod +x deploy-to-server.sh
        : "${SSH_USER:=ubuntu}"
        scp -o IdentitiesOnly=yes -o StrictHostKeyChecking=yes -i ~/.ssh/id_rsa deploy-to-server.sh ${SSH_USER}@${{ env.EC2_PUBLIC_IP }}:~/
        # Pass AWS credentials and Docker Hub credentials to EC2 for authentication
        ssh -o IdentitiesOnly=yes -o StrictHostKeyChecking=yes -i ~/.ssh/id_rsa ${SSH_USER}@${{ env.EC2_PUBLIC_IP }} "export AWS_ACCESS_KEY_ID='${{ secrets.AWS_ACCESS_KEY_ID }}' AWS_SECRET_ACCESS_KEY='${{ secrets.AWS_SECRET_ACCESS_KEY }}' AWS_DEFAULT_REGION='${{ env.AWS_REGION }}' DOCKERHUB_USERNAME='${{ secrets.DOCKERHUB_USERNAME || '' }}' DOCKERHUB_PASSWORD='${{ secrets.DOCKERHUB_PASSWORD || '' }}' ECR_REGISTRY='${{ steps.login-ecr.outputs.registry }}'; bash deploy-to-server.sh"

  # Run database migrations
  database-migration:
    name: üóÑÔ∏è Database Migration
    needs: deploy-application
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' && (github.event_name != 'workflow_dispatch' || inputs.deploy_only != true)
    env:
      EC2_PUBLIC_IP: ${{ inputs.ec2_public_ip != '' && inputs.ec2_public_ip || secrets.EC2_PUBLIC_IP }}
      RDS_ENDPOINT: ${{ inputs.rds_endpoint != '' && inputs.rds_endpoint || secrets.RDS_ENDPOINT }}
      SSH_USER: ${{ inputs.ssh_user != '' && inputs.ssh_user || secrets.EC2_SSH_USER }}
    
    steps:
    - name: üì• Checkout infrastructure repo
      uses: actions/checkout@v4
      with:
        repository: ${{ env.INFRASTRUCTURE_REPO }}
        token: ${{ secrets.GH_PAT || github.token }}
        path: infrastructure
        ref: main
        
    - name: üîê Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
        
    - name: üîë Setup SSH
      run: |
        set -e
        mkdir -p ~/.ssh
        if [ -z "${{ secrets.EC2_SSH_PRIVATE_KEY }}" ]; then echo "EC2_SSH_PRIVATE_KEY secret is required and not set."; exit 1; fi
        # Write key, handling both literal newlines and \n-escaped forms and CRLF
        rm -f ~/.ssh/id_rsa
        printf "%s" "${{ secrets.EC2_SSH_PRIVATE_KEY }}" | perl -pe 's/\r//g; s/\\n/\n/g' > ~/.ssh/id_rsa || true
        # If it still doesn't look like a key, try base64 decoding
        if ! grep -q "BEGIN .*PRIVATE KEY" ~/.ssh/id_rsa; then
          echo "Key does not appear to be raw PEM/OpenSSH. Trying base64 decode..."
          printf "%s" "${{ secrets.EC2_SSH_PRIVATE_KEY }}" | base64 -d > ~/.ssh/id_rsa 2>/dev/null || true
        fi
        chmod 600 ~/.ssh/id_rsa
        # Validate that the key can be loaded and is not passphrase-protected
        if ! ssh-keygen -y -f ~/.ssh/id_rsa > /dev/null 2>&1; then
          echo "Invalid SSH private key in secrets (format or passphrase). Ensure it is an unencrypted PEM/OpenSSH key with correct newlines.";
          exit 1
        fi
        : "${SSH_USER:=ubuntu}"
        if [ -z "${{ env.EC2_PUBLIC_IP }}" ]; then echo "EC2_PUBLIC_IP is not set. Configure the EC2_PUBLIC_IP secret."; exit 1; fi
        ssh-keyscan -H "${{ env.EC2_PUBLIC_IP }}" >> ~/.ssh/known_hosts
        
    - name: üìã Create migration script
      run: |
        cat > run-migration.sh << 'EOF'
        #!/bin/bash
        set -e
        
        echo "üóÑÔ∏è Running database migration..."
        
        # Wait for database to be ready
        for i in {1..10}; do
          if mysql -h ${{ env.RDS_ENDPOINT }} -u admin -p'${{ secrets.DB_PASSWORD }}' -e "SELECT 1;" > /dev/null 2>&1; then
            echo "‚úÖ Database connection successful!"
            break
          else
            echo "‚è≥ Attempt $i: Database not ready yet, waiting..."
            sleep 30
          fi
        done
        
        # Run migration script
        cd ~/moowi-IAC/deployment
        sudo bash setup-ec2.sh
        
        echo "‚úÖ Database migration completed!"
        EOF
        
    - name: üóÑÔ∏è Run migration on EC2
      run: |
        chmod +x run-migration.sh
        : "${SSH_USER:=ubuntu}"
        scp -o IdentitiesOnly=yes -o StrictHostKeyChecking=yes -i ~/.ssh/id_rsa run-migration.sh ${SSH_USER}@${{ env.EC2_PUBLIC_IP }}:~/
        ssh -o IdentitiesOnly=yes -o StrictHostKeyChecking=yes -i ~/.ssh/id_rsa ${SSH_USER}@${{ env.EC2_PUBLIC_IP }} "bash run-migration.sh"

  # Health check and monitoring
  health-check:
    name: üè• Health Check
    needs: database-migration
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' && (github.event_name != 'workflow_dispatch' || inputs.deploy_only != true)
    env:
      EC2_PUBLIC_IP: ${{ inputs.ec2_public_ip != '' && inputs.ec2_public_ip || secrets.EC2_PUBLIC_IP }}
      LOAD_BALANCER_DNS: ${{ inputs.load_balancer_dns != '' && inputs.load_balancer_dns || secrets.LOAD_BALANCER_DNS }}
      SSH_USER: ${{ inputs.ssh_user != '' && inputs.ssh_user || secrets.EC2_SSH_USER }}
    
    steps:
    - name: üîê Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
        
    - name: üîë Setup SSH
      run: |
        set -e
        mkdir -p ~/.ssh
        if [ -z "${{ secrets.EC2_SSH_PRIVATE_KEY }}" ]; then echo "EC2_SSH_PRIVATE_KEY secret is required and not set."; exit 1; fi
        # Write key, handling both literal newlines and \n-escaped forms and CRLF
        rm -f ~/.ssh/id_rsa
        printf "%s" "${{ secrets.EC2_SSH_PRIVATE_KEY }}" | perl -pe 's/\r//g; s/\\n/\n/g' > ~/.ssh/id_rsa || true
        # If it still doesn't look like a key, try base64 decoding
        if ! grep -q "BEGIN .*PRIVATE KEY" ~/.ssh/id_rsa; then
          echo "Key does not appear to be raw PEM/OpenSSH. Trying base64 decode..."
          printf "%s" "${{ secrets.EC2_SSH_PRIVATE_KEY }}" | base64 -d > ~/.ssh/id_rsa 2>/dev/null || true
        fi
        chmod 600 ~/.ssh/id_rsa
        # Validate that the key can be loaded and is not passphrase-protected
        if ! ssh-keygen -y -f ~/.ssh/id_rsa > /dev/null 2>&1; then
          echo "Invalid SSH private key in secrets (format or passphrase). Ensure it is an unencrypted PEM/OpenSSH key with correct newlines.";
          exit 1
        fi
        : "${SSH_USER:=ubuntu}"
        if [ -z "${{ env.EC2_PUBLIC_IP }}" ]; then echo "EC2_PUBLIC_IP is not set. Configure the EC2_PUBLIC_IP secret."; exit 1; fi
        ssh-keyscan -H "${{ env.EC2_PUBLIC_IP }}" >> ~/.ssh/known_hosts
        
    - name: üè• Run health checks
      run: |
        : "${SSH_USER:=ubuntu}"
        # Test application health
        ssh -o IdentitiesOnly=yes -o StrictHostKeyChecking=yes -i ~/.ssh/id_rsa ${SSH_USER}@${{ env.EC2_PUBLIC_IP }} "curl -f http://localhost:5000/health" || exit 1
        
        # Test Grafana
        ssh -o IdentitiesOnly=yes -o StrictHostKeyChecking=yes -i ~/.ssh/id_rsa ${SSH_USER}@${{ env.EC2_PUBLIC_IP }} "curl -f http://localhost:3000" || echo "‚ö†Ô∏è Grafana not responding"
        
        # Test Prometheus
        ssh -o IdentitiesOnly=yes -o StrictHostKeyChecking=yes -i ~/.ssh/id_rsa ${SSH_USER}@${{ env.EC2_PUBLIC_IP }} "curl -f http://localhost:9090" || echo "‚ö†Ô∏è Prometheus not responding"
        
        # Test load balancer
        curl -f http://${{ env.LOAD_BALANCER_DNS }}/health || echo "‚ö†Ô∏è Load balancer not responding"
        
    - name: üìä Check container status
      run: |
        : "${SSH_USER:=ubuntu}"
        ssh -o IdentitiesOnly=yes -o StrictHostKeyChecking=yes -i ~/.ssh/id_rsa ${SSH_USER}@${{ env.EC2_PUBLIC_IP }} "sudo docker ps"
        
    - name: üìù Deployment Summary
      run: |
        echo "üéâ Deployment completed successfully!"
        echo "üåê Application URL: http://${{ env.LOAD_BALANCER_DNS }}"
        echo "üìä Grafana URL: http://${{ env.EC2_PUBLIC_IP }}:3000"
        echo "üìà Prometheus URL: http://${{ env.EC2_PUBLIC_IP }}:9090"
        echo "üóÑÔ∏è Database: ${{ env.RDS_ENDPOINT }}"

  # Notify on completion
  notify:
    name: üì¢ Notify
    needs: [health-check]
    runs-on: ubuntu-latest
    if: always() && github.ref == 'refs/heads/main'
    
    steps:
    - name: üìß Send notification
      run: |
        if [ "${{ needs.health-check.result }}" == "success" ]; then
          echo "‚úÖ Deployment successful!"
          # Add your notification logic here (Slack, email, etc.)
        else
          echo "‚ùå Deployment failed!"
          # Add your notification logic here
        fi 