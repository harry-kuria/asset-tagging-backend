name: üöÄ Deploy Asset Tagging Backend

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      deploy_only:
        description: 'Skip Terraform and deploy to existing EC2 only'
        required: false
        type: boolean
        default: false
      ec2_public_ip:
        description: 'EC2 public IP for deploy-only runs (overrides secret if provided)'
        required: false
        type: string
        default: ''
      rds_endpoint:
        description: 'RDS endpoint for migration/health (overrides secret if provided)'
        required: false
        type: string
        default: ''
      load_balancer_dns:
        description: 'ALB DNS for health checks (overrides secret if provided)'
        required: false
        type: string
        default: ''
      ssh_user:
        description: 'SSH username for EC2 (e.g., ubuntu, ec2-user)'
        required: false
        type: string
        default: 'ubuntu'

env:
  AWS_REGION: us-east-1
  ECR_REPOSITORY: asset-tagging-backend
  INFRASTRUCTURE_REPO: harry-kuria/moowi-IAC

jobs:
  # Test the backend application
  test:
    name: üß™ Test Backend
    runs-on: ubuntu-latest
    
    steps:
    - name: üì• Checkout code
      uses: actions/checkout@v4
      
    - name: üêπ Setup Go
      uses: actions/setup-go@v4
      with:
        go-version: '1.22.x'
        cache: true
        
    - name: üì¶ Install dependencies
      run: go mod download
      
    - name: üß™ Run tests
      run: go test -v ./...
      
    - name: üîç Run linting
      run: |
        go install golang.org/x/lint/golint@latest
        golint -set_exit_status ./...
        
    - name: üîí Run security scan
      run: |
        go env -w GOFLAGS='-buildvcs=false'
        go install github.com/securego/gosec/v2/cmd/gosec@v2.20.0
        echo "$HOME/go/bin" >> $GITHUB_PATH
        gosec ./...

  # Build and push Docker image
  build:
    name: üê≥ Build & Push Docker Image
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop' || github.event_name == 'workflow_dispatch'
    
    steps:
    - name: üì• Checkout code
      uses: actions/checkout@v4
      
    - name: üîê Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: üèóÔ∏è Ensure ECR repository exists and is mutable
      run: |
        aws ecr describe-repositories --repository-names $ECR_REPOSITORY >/dev/null 2>&1 || \
        aws ecr create-repository \
          --repository-name $ECR_REPOSITORY \
          --image-tag-mutability MUTABLE \
          --image-scanning-configuration scanOnPush=true
        # Force mutability in case repo already existed and was immutable
        aws ecr put-image-tag-mutability \
          --repository-name $ECR_REPOSITORY \
          --image-tag-mutability MUTABLE || true
      
    - name: üîë Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2
      
    - name: üèóÔ∏è Build, tag, and push image to Amazon ECR
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        IMAGE_TAG: ${{ github.sha }}
      run: |
        docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG .
        docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:latest .
        docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
        docker push $ECR_REGISTRY/$ECR_REPOSITORY:latest
        echo "image=$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG" >> $GITHUB_OUTPUT

  # Deploy to production
  deploy:
    name: üö´ Provision Infrastructure (disabled)
    needs: build
    runs-on: ubuntu-latest
    if: false
    
    steps:
    - name: üì• Checkout infrastructure repo
      uses: actions/checkout@v4
      with:
        repository: ${{ env.INFRASTRUCTURE_REPO }}
        token: ${{ secrets.GH_PAT || github.token }}
        path: infrastructure
        ref: main
        
    - name: üîê Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
        
    - name: üßπ Pre-clean conflicting named resources (safe, idempotent)
      run: |
        set -e
        echo "Checking for pre-existing resources that may conflict with names..."
        ALB_ARN=$(aws elbv2 describe-load-balancers --region $AWS_REGION --names asset-tagging-alb --query 'LoadBalancers[0].LoadBalancerArn' --output text 2>/dev/null || echo "None")
        TG_ARN=$(aws elbv2 describe-target-groups --region $AWS_REGION --names asset-tagging-tg --query 'TargetGroups[0].TargetGroupArn' --output text 2>/dev/null || echo "None")
        # Delete listeners if ALB exists
        if [ "$ALB_ARN" != "None" ] && [ -n "$ALB_ARN" ]; then
          echo "Found existing ALB: $ALB_ARN. Deleting listeners..."
          LISTENERS=$(aws elbv2 describe-listeners --region $AWS_REGION --load-balancer-arn "$ALB_ARN" --query 'Listeners[].ListenerArn' --output text 2>/dev/null || true)
          for L in $LISTENERS; do
            aws elbv2 delete-listener --region $AWS_REGION --listener-arn "$L" || true
          done
          echo "Deleting ALB..."
          aws elbv2 delete-load-balancer --region $AWS_REGION --load-balancer-arn "$ALB_ARN" || true
          aws elbv2 wait load-balancer-deleted --region $AWS_REGION --load-balancer-arns "$ALB_ARN" || true
        else
          echo "No pre-existing ALB found."
        fi
        # Delete target group
        if [ "$TG_ARN" != "None" ] && [ -n "$TG_ARN" ]; then
          echo "Found existing TG: $TG_ARN. Deregistering targets and deleting TG..."
          aws elbv2 modify-target-group-attributes --region $AWS_REGION --target-group-arn "$TG_ARN" --attributes Key=deregistration_delay.timeout_seconds,Value=30 || true
          TARGET_IDS=$(aws elbv2 describe-target-health --region $AWS_REGION --target-group-arn "$TG_ARN" --query 'TargetHealthDescriptions[].Target.Id' --output text 2>/dev/null || true)
          for id in $TARGET_IDS; do
            aws elbv2 deregister-targets --region $AWS_REGION --target-group-arn "$TG_ARN" --targets Id=$id || true
          done
          sleep 10
          aws elbv2 delete-target-group --region $AWS_REGION --target-group-arn "$TG_ARN" || true
        else
          echo "No pre-existing TG found."
        fi
        # Delete DB subnet group
        echo "Attempting to delete pre-existing DB subnet group (if any)..."
        aws rds delete-db-subnet-group --region $AWS_REGION --db-subnet-group-name asset-tagging-db-subnet-group || true
        echo "Pre-clean complete."
        
    - name: üì¶ Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: "1.5.0"
        
    - name: üîç Terraform Format Check
      working-directory: ./infrastructure/terraform
      run: |
        terraform fmt -recursive
        terraform fmt -recursive -check -diff
      
    - name: ‚úÖ Terraform Init
      working-directory: ./infrastructure/terraform
      env:
        TF_IN_AUTOMATION: 'true'
        TF_VAR_db_password: ${{ secrets.DB_PASSWORD }}
        TF_VAR_ssh_key_name: ${{ secrets.SSH_KEY_NAME }}
        TF_VAR_instance_type: 't2.micro'
        TF_VAR_manage_db: 'false'
      run: terraform init
      
    - name: üîé Discover existing DB instance (if any)
      run: |
        set -e
        echo "Checking for pre-existing RDS instance with identifier asset-tagging-db..."
        DB_EXISTS=$(aws rds describe-db-instances --region $AWS_REGION --db-instance-identifier asset-tagging-db >/dev/null 2>&1 && echo yes || echo no)
        echo "DB_EXISTS=${DB_EXISTS}" >> $GITHUB_ENV
      
    - name: üß≠ Import existing DB instance (one-time)
      if: env.DB_EXISTS == 'yes'
      working-directory: ./infrastructure/terraform
      env:
        TF_IN_AUTOMATION: 'true'
        TF_INPUT: 'false'
        TF_VAR_db_password: ${{ secrets.DB_PASSWORD }}
        TF_VAR_ssh_key_name: ${{ secrets.SSH_KEY_NAME }}
      run: |
        terraform import aws_db_instance.main asset-tagging-db || true
        terraform init -reconfigure || true
      
    - name: üìã Terraform Plan
      working-directory: ./infrastructure/terraform
      env:
        TF_IN_AUTOMATION: 'true'
        TF_VAR_db_password: ${{ secrets.DB_PASSWORD }}
        TF_VAR_ssh_key_name: ${{ secrets.SSH_KEY_NAME }}
        TF_VAR_instance_type: 't2.micro'
        TF_VAR_manage_db: 'false'
      run: terraform plan -input=false -out=tfplan
      
    - name: üöÄ Terraform Apply
      working-directory: ./infrastructure/terraform
      env:
        TF_IN_AUTOMATION: 'true'
        TF_VAR_db_password: ${{ secrets.DB_PASSWORD }}
        TF_VAR_ssh_key_name: ${{ secrets.SSH_KEY_NAME }}
        TF_VAR_instance_type: 't2.micro'
        TF_VAR_manage_db: 'false'
      run: terraform apply -input=false -auto-approve tfplan
      
    # Terraform outputs step removed because provisioning is disabled

  # Deploy application to EC2
  deploy-application:
    name: üöÄ Deploy Application
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch'
    env:
      EC2_PUBLIC_IP: ${{ inputs.ec2_public_ip != '' && inputs.ec2_public_ip || secrets.EC2_PUBLIC_IP }}
      SSH_USER: ${{ inputs.ssh_user != '' && inputs.ssh_user || secrets.EC2_SSH_USER }}
    
    steps:
    - name: üì• Checkout infrastructure repo
      uses: actions/checkout@v4
      with:
        repository: ${{ env.INFRASTRUCTURE_REPO }}
        token: ${{ secrets.GH_PAT || github.token }}
        path: infrastructure
        ref: main
        
    - name: üîê Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
        
    - name: üîë Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2
      
    - name: üîë Setup SSH
      run: |
        mkdir -p ~/.ssh
        if [ -z "${{ secrets.EC2_SSH_PRIVATE_KEY }}" ]; then echo "EC2_SSH_PRIVATE_KEY secret is required and not set."; exit 1; fi
        KEY_CONTENT='${{ secrets.EC2_SSH_PRIVATE_KEY }}'
        if echo "$KEY_CONTENT" | grep -q "BEGIN .*PRIVATE KEY"; then
          printf "%s\n" "$KEY_CONTENT" | sed 's/\r$//' > ~/.ssh/id_rsa
        else
          printf "%s" "$KEY_CONTENT" | tr -d '\r' | base64 -d --ignore-garbage > ~/.ssh/id_rsa 2>/dev/null || { echo "EC2_SSH_PRIVATE_KEY is neither a valid PEM nor valid base64-encoded PEM."; exit 1; }
        fi
        chmod 600 ~/.ssh/id_rsa
        ssh-keygen -y -f ~/.ssh/id_rsa > /dev/null || { echo "Invalid SSH private key in secrets (format or passphrase)."; exit 1; }
        if [ -z "${{ env.EC2_PUBLIC_IP }}" ]; then echo "EC2_PUBLIC_IP is not set. Configure the EC2_PUBLIC_IP secret."; exit 1; fi
        : "${SSH_USER:=ubuntu}"
        ssh-keyscan -H "${{ env.EC2_PUBLIC_IP }}" >> ~/.ssh/known_hosts
        
    - name: üìã Create deployment script
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
      run: |
        cat > deploy-to-server.sh << 'EOF'
        #!/bin/bash
        set -e
        
        echo "üöÄ Starting deployment..."
        
        # Update infrastructure repository
        cd ~/moowi-IAC
        git pull origin main || true
        
        # Stop current services
        cd /opt/asset-tagging
        sudo docker-compose down || true

        echo "üßπ Pruning unused Docker images and containers to free space..."
        sudo docker system df || true
        sudo docker container prune -f || true
        sudo docker image prune -af || true
        sudo docker system df || true
        
        # Pull latest image
        sudo docker pull ${ECR_REGISTRY}/${ECR_REPOSITORY}:latest
        
        # Update docker-compose.yml with new image
        sudo sed -i "s|image: .*asset-tagging-backend.*|image: ${ECR_REGISTRY}/${ECR_REPOSITORY}:latest|g" docker-compose.yml
        
        # Start services
        sudo docker-compose up -d
        
        echo "‚è≥ Waiting for services to be ready..."
        for i in {1..10}; do
          if curl -fsS http://localhost:5000/health > /dev/null; then
            echo "‚úÖ Application deployed successfully!"
            exit 0
          fi
          echo "Attempt $i/10: service not ready yet, waiting 10s..."
          sleep 10
        done
        echo "‚ùå Application health check failed after retries!"
        exit 1
        EOF
        
    - name: üöÄ Deploy to EC2
      run: |
        chmod +x deploy-to-server.sh
        : "${SSH_USER:=ubuntu}"
        scp -o IdentitiesOnly=yes -o StrictHostKeyChecking=yes -i ~/.ssh/id_rsa deploy-to-server.sh ${SSH_USER}@${{ env.EC2_PUBLIC_IP }}:~/
        ssh -o IdentitiesOnly=yes -o StrictHostKeyChecking=yes -i ~/.ssh/id_rsa ${SSH_USER}@${{ env.EC2_PUBLIC_IP }} "bash deploy-to-server.sh"

  # Run database migrations
  database-migration:
    name: üóÑÔ∏è Database Migration
    needs: deploy-application
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' && (github.event_name != 'workflow_dispatch' || inputs.deploy_only != true)
    env:
      EC2_PUBLIC_IP: ${{ inputs.ec2_public_ip != '' && inputs.ec2_public_ip || secrets.EC2_PUBLIC_IP }}
      RDS_ENDPOINT: ${{ inputs.rds_endpoint != '' && inputs.rds_endpoint || secrets.RDS_ENDPOINT }}
      SSH_USER: ${{ inputs.ssh_user != '' && inputs.ssh_user || secrets.EC2_SSH_USER }}
    
    steps:
    - name: üì• Checkout infrastructure repo
      uses: actions/checkout@v4
      with:
        repository: ${{ env.INFRASTRUCTURE_REPO }}
        token: ${{ secrets.GH_PAT || github.token }}
        path: infrastructure
        ref: main
        
    - name: üîê Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
        
    - name: üîë Setup SSH
      run: |
        mkdir -p ~/.ssh
        if [ -z "${{ secrets.EC2_SSH_PRIVATE_KEY }}" ]; then echo "EC2_SSH_PRIVATE_KEY secret is required and not set."; exit 1; fi
        KEY_CONTENT='${{ secrets.EC2_SSH_PRIVATE_KEY }}'
        if echo "$KEY_CONTENT" | grep -q "BEGIN .*PRIVATE KEY"; then
          printf "%s\n" "$KEY_CONTENT" | sed 's/\r$//' > ~/.ssh/id_rsa
        else
          printf "%s" "$KEY_CONTENT" | tr -d '\r' | base64 -d --ignore-garbage > ~/.ssh/id_rsa 2>/dev/null || { echo "EC2_SSH_PRIVATE_KEY is neither a valid PEM nor valid base64-encoded PEM."; exit 1; }
        fi
        chmod 600 ~/.ssh/id_rsa
        ssh-keygen -y -f ~/.ssh/id_rsa > /dev/null || { echo "Invalid SSH private key in secrets (format or passphrase)."; exit 1; }
        if [ -z "${{ env.EC2_PUBLIC_IP }}" ]; then echo "EC2_PUBLIC_IP is not set. Configure the EC2_PUBLIC_IP secret."; exit 1; fi
        : "${SSH_USER:=ubuntu}"
        ssh-keyscan -H "${{ env.EC2_PUBLIC_IP }}" >> ~/.ssh/known_hosts
        
    - name: üìã Create migration script
      run: |
        cat > run-migration.sh << 'EOF'
        #!/bin/bash
        set -e
        
        echo "üóÑÔ∏è Running database migration..."
        
        # Wait for database to be ready
        for i in {1..10}; do
          if mysql -h ${{ env.RDS_ENDPOINT }} -u admin -p'${{ secrets.DB_PASSWORD }}' -e "SELECT 1;" > /dev/null 2>&1; then
            echo "‚úÖ Database connection successful!"
            break
          else
            echo "‚è≥ Attempt $i: Database not ready yet, waiting..."
            sleep 30
          fi
        done
        
        # Run migration script
        cd ~/moowi-IAC/deployment
        sudo bash setup-ec2.sh
        
        echo "‚úÖ Database migration completed!"
        EOF
        
    - name: üóÑÔ∏è Run migration on EC2
      run: |
        chmod +x run-migration.sh
        : "${SSH_USER:=ubuntu}"
        scp -o IdentitiesOnly=yes -o StrictHostKeyChecking=yes -i ~/.ssh/id_rsa run-migration.sh ${SSH_USER}@${{ env.EC2_PUBLIC_IP }}:~/
        ssh -o IdentitiesOnly=yes -o StrictHostKeyChecking=yes -i ~/.ssh/id_rsa ${SSH_USER}@${{ env.EC2_PUBLIC_IP }} "bash run-migration.sh"

  # Health check and monitoring
  health-check:
    name: üè• Health Check
    needs: database-migration
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' && (github.event_name != 'workflow_dispatch' || inputs.deploy_only != true)
    env:
      EC2_PUBLIC_IP: ${{ inputs.ec2_public_ip != '' && inputs.ec2_public_ip || secrets.EC2_PUBLIC_IP }}
      LOAD_BALANCER_DNS: ${{ inputs.load_balancer_dns != '' && inputs.load_balancer_dns || secrets.LOAD_BALANCER_DNS }}
      SSH_USER: ${{ inputs.ssh_user != '' && inputs.ssh_user || secrets.EC2_SSH_USER }}
    
    steps:
    - name: üîê Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
        
    - name: üîë Setup SSH
      run: |
        mkdir -p ~/.ssh
        if [ -z "${{ secrets.EC2_SSH_PRIVATE_KEY }}" ]; then echo "EC2_SSH_PRIVATE_KEY secret is required and not set."; exit 1; fi
        KEY_CONTENT='${{ secrets.EC2_SSH_PRIVATE_KEY }}'
        if echo "$KEY_CONTENT" | grep -q "BEGIN .*PRIVATE KEY"; then
          printf "%s\n" "$KEY_CONTENT" | sed 's/\r$//' > ~/.ssh/id_rsa
        else
          printf "%s" "$KEY_CONTENT" | tr -d '\r' | base64 -d --ignore-garbage > ~/.ssh/id_rsa 2>/dev/null || { echo "EC2_SSH_PRIVATE_KEY is neither a valid PEM nor valid base64-encoded PEM."; exit 1; }
        fi
        chmod 600 ~/.ssh/id_rsa
        ssh-keygen -y -f ~/.ssh/id_rsa > /dev/null || { echo "Invalid SSH private key in secrets (format or passphrase)."; exit 1; }
        if [ -z "${{ env.EC2_PUBLIC_IP }}" ]; then echo "EC2_PUBLIC_IP is not set. Configure the EC2_PUBLIC_IP secret."; exit 1; fi
        : "${SSH_USER:=ubuntu}"
        ssh-keyscan -H "${{ env.EC2_PUBLIC_IP }}" >> ~/.ssh/known_hosts
        
    - name: üè• Run health checks
      run: |
        : "${SSH_USER:=ubuntu}"
        # Test application health
        ssh -o IdentitiesOnly=yes -o StrictHostKeyChecking=yes -i ~/.ssh/id_rsa ${SSH_USER}@${{ env.EC2_PUBLIC_IP }} "curl -f http://localhost:5000/health" || exit 1
        
        # Test Grafana
        ssh -o IdentitiesOnly=yes -o StrictHostKeyChecking=yes -i ~/.ssh/id_rsa ${SSH_USER}@${{ env.EC2_PUBLIC_IP }} "curl -f http://localhost:3000" || echo "‚ö†Ô∏è Grafana not responding"
        
        # Test Prometheus
        ssh -o IdentitiesOnly=yes -o StrictHostKeyChecking=yes -i ~/.ssh/id_rsa ${SSH_USER}@${{ env.EC2_PUBLIC_IP }} "curl -f http://localhost:9090" || echo "‚ö†Ô∏è Prometheus not responding"
        
        # Test load balancer
        curl -f http://${{ env.LOAD_BALANCER_DNS }}/health || echo "‚ö†Ô∏è Load balancer not responding"
        
    - name: üìä Check container status
      run: |
        : "${SSH_USER:=ubuntu}"
        ssh -o IdentitiesOnly=yes -o StrictHostKeyChecking=yes -i ~/.ssh/id_rsa ${SSH_USER}@${{ env.EC2_PUBLIC_IP }} "sudo docker ps"
        
    - name: üìù Deployment Summary
      run: |
        echo "üéâ Deployment completed successfully!"
        echo "üåê Application URL: http://${{ env.LOAD_BALANCER_DNS }}"
        echo "üìä Grafana URL: http://${{ env.EC2_PUBLIC_IP }}:3000"
        echo "üìà Prometheus URL: http://${{ env.EC2_PUBLIC_IP }}:9090"
        echo "üóÑÔ∏è Database: ${{ env.RDS_ENDPOINT }}"

  # Notify on completion
  notify:
    name: üì¢ Notify
    needs: [health-check]
    runs-on: ubuntu-latest
    if: always() && github.ref == 'refs/heads/main'
    
    steps:
    - name: üìß Send notification
      run: |
        if [ "${{ needs.health-check.result }}" == "success" ]; then
          echo "‚úÖ Deployment successful!"
          # Add your notification logic here (Slack, email, etc.)
        else
          echo "‚ùå Deployment failed!"
          # Add your notification logic here
        fi 